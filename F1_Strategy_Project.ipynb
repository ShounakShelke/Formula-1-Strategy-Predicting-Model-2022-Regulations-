{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a03aa8b8",
   "metadata": {},
   "source": [
    "\n",
    "# F1 Strategy ML Project — End-to-End Notebook\n",
    "**Goal:** Build models for pit-stop and tyre strategy prediction using FastF1/OpenF1 and tabular & sequence models.\n",
    "This notebook provides a complete pipeline and strong modelling practices (feature engineering, imbalanced handling,\n",
    "Optuna hyperparameter tuning, stacking ensembles, calibration, SHAP explainability) designed to *aim* for high accuracy.\n",
    "**Important:** I cannot train models or access external APIs from this environment. Run this notebook locally or in Google Colab to execute data ingestion and training.\n",
    "\n",
    "**How to use**\n",
    "1. Run Part 1 to install packages and ingest data (FastF1 or local CSVs).\n",
    "2. Follow Parts 2–6 sequentially.\n",
    "3. Use GPU runtime in Colab for sequence models (LSTM).\n",
    "4. Target accuracy: the notebook contains techniques meant to reach high performance; actual achieved accuracy depends on data quality and hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6efe674",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1 — Environment setup & Data ingestion\n",
    "\n",
    "**What this cell does**\n",
    "- Installs required packages (uncomment for Colab)\n",
    "- Sets up directory structure\n",
    "- Shows how to load FastF1 sessions or local CSV/parquet\n",
    "\n",
    "**NOTE:** In Colab, uncomment the `!pip install` line. Locally, install packages in your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8ad4620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastf1\n",
      "  Using cached fastf1-3.6.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xgboost\n",
      "  Using cached xgboost-3.1.1-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: lightgbm in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.6.0)\n",
      "Requirement already satisfied: optuna in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.5.0)\n",
      "Requirement already satisfied: shap in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.49.1)\n",
      "Collecting imbalanced-learn\n",
      "  Using cached imbalanced_learn-0.14.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.7.2)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.20.0-cp311-cp311-win_amd64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.10.1)\n",
      "Requirement already satisfied: seaborn in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.13.2)\n",
      "Requirement already satisfied: nbformat in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (5.10.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.5.2)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.32.3)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.23.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from fastf1) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from fastf1) (2.9.0.post0)\n",
      "Requirement already satisfied: rapidfuzz in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from fastf1) (3.14.3)\n",
      "Collecting requests-cache>=1.0.0 (from fastf1)\n",
      "  Using cached requests_cache-1.2.1-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.8.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from fastf1) (1.15.2)\n",
      "Collecting timple>=0.1.6 (from fastf1)\n",
      "  Using cached timple-0.1.8-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: websockets<14,>=10.3 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from fastf1) (13.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from optuna) (1.17.1)\n",
      "Requirement already satisfied: colorlog in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from optuna) (6.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from optuna) (2.0.44)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: slicer==0.0.8 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba>=0.54 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from shap) (0.62.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from shap) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from shap) (4.15.0)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from imbalanced-learn) (3.6.0)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (25.9.23)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (18.1.1)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (6.33.0)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (3.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow) (2.0.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Using cached grpcio-1.76.0-cp311-cp311-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Using cached tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow)\n",
      "  Using cached keras-3.12.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Using cached h5py-3.15.1-cp311-cp311-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Using cached ml_dtypes-0.5.3-cp311-cp311-win_amd64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nbformat) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nbformat) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nbformat) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nbformat) (5.14.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: Mako in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonschema>=2.6->nbformat) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonschema>=2.6->nbformat) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonschema>=2.6->nbformat) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonschema>=2.6->nbformat) (0.23.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.3.6)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (309)\n",
      "Collecting rich (from keras>=3.10.0->tensorflow)\n",
      "  Using cached rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: namex in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow)\n",
      "  Using cached optree-0.17.0-cp311-cp311-win_amd64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from numba>=0.54->shap) (0.45.1)\n",
      "Collecting cattrs>=22.2 (from requests-cache>=1.0.0->fastf1)\n",
      "  Using cached cattrs-25.3.0-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: url-normalize>=1.4 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests-cache>=1.0.0->fastf1) (2.2.1)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Using cached markdown-3.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm->optuna) (0.4.6)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=2.6->nbformat)\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.10.0->tensorflow)\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached fastf1-3.6.1-py3-none-any.whl (148 kB)\n",
      "Using cached xgboost-3.1.1-py3-none-win_amd64.whl (72.0 MB)\n",
      "Using cached imbalanced_learn-0.14.0-py3-none-any.whl (239 kB)\n",
      "Using cached tensorflow-2.20.0-cp311-cp311-win_amd64.whl (331.8 MB)\n",
      "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached grpcio-1.76.0-cp311-cp311-win_amd64.whl (4.7 MB)\n",
      "Using cached h5py-3.15.1-cp311-cp311-win_amd64.whl (2.9 MB)\n",
      "Using cached keras-3.12.0-py3-none-any.whl (1.5 MB)\n",
      "Using cached ml_dtypes-0.5.3-cp311-cp311-win_amd64.whl (206 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached requests_cache-1.2.1-py3-none-any.whl (61 kB)\n",
      "Using cached tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached timple-0.1.8-py3-none-any.whl (17 kB)\n",
      "Using cached cattrs-25.3.0-py3-none-any.whl (70 kB)\n",
      "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Using cached markdown-3.10-py3-none-any.whl (107 kB)\n",
      "Using cached optree-0.17.0-cp311-cp311-win_amd64.whl (313 kB)\n",
      "Using cached rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: optree, opt_einsum, ml_dtypes, mdurl, markdown, h5py, grpcio, google_pasta, gast, attrs, astunparse, absl-py, xgboost, tensorboard, markdown-it-py, cattrs, timple, rich, requests-cache, imbalanced-learn, keras, fastf1, tensorflow\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 25.4.0\n",
      "    Uninstalling attrs-25.4.0:\n",
      "      Successfully uninstalled attrs-25.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\HP\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python311\\\\site-packages\\\\tensorflow\\\\include\\\\external\\\\com_github_grpc_grpc\\\\src\\\\core\\\\ext\\\\filters\\\\fault_injection\\\\fault_injection_service_config_parser.h'\n",
      "HINT: This error might have occurred since this system does not have Windows Long Path support enabled. You can find information on how to enable this at https://pip.pypa.io/warnings/enable-long-paths\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\HP\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install fastf1 xgboost lightgbm optuna shap imbalanced-learn scikit-learn tensorflow pandas matplotlib seaborn nbformat joblib requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6428e4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   meeting_key  session_key  driver_number  lap_number  \\\n",
      "0         1273         9869              1           1   \n",
      "1         1273         9869              1           2   \n",
      "2         1273         9869              1           3   \n",
      "3         1273         9869              1           4   \n",
      "4         1273         9869              1           5   \n",
      "\n",
      "                         date_start  duration_sector_1  duration_sector_2  \\\n",
      "0                              None                NaN             39.288   \n",
      "1  2025-11-09T17:04:41.604000+00:00             21.320             75.859   \n",
      "2  2025-11-09T17:06:41.982000+00:00             21.675             58.682   \n",
      "3  2025-11-09T17:08:30.947000+00:00             25.318             59.748   \n",
      "4  2025-11-09T17:10:17.731000+00:00             30.078             50.463   \n",
      "\n",
      "   duration_sector_3  i1_speed  i2_speed  is_pit_out_lap  lap_duration  \\\n",
      "0             17.131     306.0     244.0           False           NaN   \n",
      "1             23.234     120.0     199.0           False       120.413   \n",
      "2             28.650     167.0     197.0           False       109.007   \n",
      "3             21.554     236.0     221.0           False       106.620   \n",
      "4             27.348     250.0     229.0           False       107.889   \n",
      "\n",
      "                            segments_sector_1  \\\n",
      "0  [None, 2064, 2064, 2064, 2064, 2064, 2049]   \n",
      "1  [2049, 2049, 2049, 2049, 2049, 2049, 2048]   \n",
      "2  [2048, 2048, 2049, 2049, 2048, 2048, 2048]   \n",
      "3  [None, 2048, 2048, 2048, 2048, 2048, 2048]   \n",
      "4  [None, 2048, 2048, 2048, 2048, 2048, 2048]   \n",
      "\n",
      "                                  segments_sector_2  \\\n",
      "0  [2049, 2049, 2049, 2049, 2049, 2049, 2049, 2049]   \n",
      "1  [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048]   \n",
      "2  [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048]   \n",
      "3  [2048, 2048, 2048, 2048, 2048, 2048, 2049, 2048]   \n",
      "4  [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048]   \n",
      "\n",
      "                      segments_sector_3  st_speed  \n",
      "0     [2049, 2049, 2051, 2049, 2049, 0]       NaN  \n",
      "1  [2048, 2048, 2048, 2048, 2048, 2048]     271.0  \n",
      "2        [2048, 2048, 2048, 2048, 2048]     258.0  \n",
      "3  [2048, 2048, 2048, 2048, 2048, 2048]     237.0  \n",
      "4  [2048, 2048, 2048, 2048, 2048, 2048]     247.0  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Example: Get driver laps from 2024 Bahrain GP (race ID 1)\n",
    "url = \"https://api.openf1.org/v1/laps?session_key=latest&driver_number=1\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    df = pd.DataFrame(data)\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"Error:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72615798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR = \\content\\f1_data\n",
      "CACHE_DIR = \\content\\fastf1_cache\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "DATA_DIR = Path('/content/f1_data')  # change to your preferred path (Colab default)\n",
    "CACHE_DIR = Path('/content/fastf1_cache')\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('DATA_DIR =', DATA_DIR)\n",
    "print('CACHE_DIR =', CACHE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b35fda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastF1 load failed; ensure fastf1 installed and internet access available. Error: name 'Monaco' is not defined\n",
      "You can also provide local CSV/parquet files in DATA_DIR and skip FastF1.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# FastF1 quick ingestion example. Requires internet access and fastf1 package.\n",
    "# Replace YEAR, GP, SESSION as needed. If running in Colab, run the pip install cell above first.\n",
    "\n",
    "try:\n",
    "    import fastf1\n",
    "    fastf1.Cache.enable_cache(str(CACHE_DIR))\n",
    "    YEAR = 2023\n",
    "    GP = 'Monaco'\n",
    "    SESSION = 'R'  # 'FP1','FP2','Q','R'\n",
    "    session = fastf1.get_session(2023, Monaco, R)\n",
    "    session.load(laps=True, telemetry=False)  # telemetry can be heavy; toggle as needed\n",
    "    laps = session.laps\n",
    "    print(f'Loaded {len(laps)} laps from {2023} {Monaco} {R}')\n",
    "    # Save to parquet for reproducibility\n",
    "    laps.to_parquet(DATA_DIR / f'laps_{2023}_{Monaco}_{R}.parquet')\n",
    "except Exception as e:\n",
    "    print('FastF1 load failed; ensure fastf1 installed and internet access available. Error:', e)\n",
    "    print('You can also provide local CSV/parquet files in DATA_DIR and skip FastF1.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832444d5",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2 — Feature Engineering & Labeling (Detailed)\n",
    "This section builds canonical per-lap features and task labels:\n",
    "- `pit_next` (binary): whether the car pits next lap\n",
    "- `next_lap_delta` (regression): next-lap time delta (proxy for tyre degradation)\n",
    "- Additional recommended features: gap to leader, competitor tyre ages, ambient/track temps, stint-length, compound encodings\n",
    "\n",
    "The code below expects a `laps` DataFrame (from FastF1 or local parquet).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3d51a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# load saved laps parquet (from Part 1) or local CSV\n",
    "# adapt filename to your saved files\n",
    "parquet_candidates = list(DATA_DIR.glob('laps_*.parquet'))\n",
    "if parquet_candidates:\n",
    "    laps_df = pd.read_parquet(parquet_candidates[0])\n",
    "    print('Loaded', parquet_candidates[0])\n",
    "else:\n",
    "    raise FileNotFoundError('No laps parquet found in DATA_DIR. Run the FastF1 cell or upload data.')\n",
    "\n",
    "# Basic cleaning and canonical features\n",
    "# Keep only accurate laps\n",
    "if 'IsAccurate' in laps_df.columns:\n",
    "    laps_df = laps_df[laps_df['IsAccurate'] == True].copy()\n",
    "\n",
    "# Lap time in seconds\n",
    "if 'LapTime' in laps_df.columns:\n",
    "    laps_df['lap_sec'] = laps_df['LapTime'].dt.total_seconds()\n",
    "else:\n",
    "    # fallback if lap time already float\n",
    "    if 'lap_sec' not in laps_df.columns:\n",
    "        raise KeyError('LapTime column not found; please ensure laps dataframe contains LapTime or lap_sec.')\n",
    "\n",
    "# Tyre age within stint\n",
    "laps_df['Stint'] = laps_df['Stint'].fillna(0).astype(int)\n",
    "laps_df['tyre_age'] = laps_df.groupby(['Driver', 'Stint']).cumcount() + 1\n",
    "\n",
    "# Lap delta and rolling features\n",
    "laps_df['prev_lap_sec'] = laps_df.groupby('Driver')['lap_sec'].shift(1)\n",
    "laps_df['lap_delta'] = laps_df['lap_sec'] - laps_df['prev_lap_sec']\n",
    "laps_df['rolling_mean_3'] = laps_df.groupby('Driver')['lap_sec'].rolling(3, min_periods=1).mean().reset_index(0,drop=True)\n",
    "\n",
    "# Weather join placeholder (FastF1 provides session.weather)\n",
    "# If session available: session.weather['AmbientTemperature'] or session.weather['TrackTemperature']\n",
    "# For local CSVs, include columns ambient_temp, track_temp per lap if available.\n",
    "\n",
    "# Label: pit_next (needs pit stop table)\n",
    "pit_next = np.zeros(len(laps_df), dtype=int)\n",
    "try:\n",
    "    # FastF1 pit stops may exist: session.load_pit_data()\n",
    "    pitstops = session.load_pit_data()\n",
    "    pits = {(r['Driver'], int(r['Lap'])) for r in pitstops.to_dict('records')}\n",
    "    for i, row in laps_df.iterrows():\n",
    "        if (row['Driver'], int(row['LapNumber'])+1) in pits:\n",
    "            pit_next[i] = 1\n",
    "    laps_df['pit_next'] = pit_next\n",
    "    print('pit_next labels created; positives =', laps_df['pit_next'].sum())\n",
    "except Exception as e:\n",
    "    print('Pit stop data not available; create pit_next from external pit CSV or set manually. Error:', e)\n",
    "    laps_df['pit_next'] = 0\n",
    "\n",
    "# Regression target: next_lap_delta as a proxy for tyre degradation\n",
    "laps_df['next_lap_sec'] = laps_df.groupby('Driver')['lap_sec'].shift(-1)\n",
    "laps_df['next_lap_delta'] = laps_df['next_lap_sec'] - laps_df['lap_sec']\n",
    "\n",
    "# Save features for modeling\n",
    "FEATURES_PATH = DATA_DIR / 'features_parquet.parquet'\n",
    "cols_to_save = ['Driver', 'Team', 'LapNumber', 'Stint', 'Compound', 'tyre_age', 'lap_sec', 'lap_delta',\n",
    "                'rolling_mean_3', 'pit_next', 'next_lap_delta']\n",
    "# Some columns may be missing depending on data; filter\n",
    "cols_to_save = [c for c in cols_to_save if c in laps_df.columns]\n",
    "laps_df[cols_to_save].to_parquet(FEATURES_PATH)\n",
    "print('Saved features to', FEATURES_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bd1ddd",
   "metadata": {},
   "source": [
    "\n",
    "## Part 3 — Baseline Modeling + Hyperparameter Tuning (Optuna)\n",
    "We build:\n",
    "- Pit-next classifier (XGBoost) with Optuna tuning and cross-validation\n",
    "- Tyre degradation regressor (XGBoost) with Optuna tuning\n",
    "\n",
    "We'll include handling class imbalance (scale_pos_weight or SMOTE) and stacking ensemble later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4b5a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, mean_squared_error, mean_absolute_error\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "FEATURES_PATH = DATA_DIR / 'features_parquet.parquet'\n",
    "df = pd.read_parquet(FEATURES_PATH)\n",
    "df = df.dropna(subset=['lap_sec'])\n",
    "\n",
    "# Encode categoricals\n",
    "for col in ['Compound', 'Driver', 'Team']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str)\n",
    "        df[col + '_enc'] = LabelEncoder().fit_transform(df[col])\n",
    "\n",
    "# Common features\n",
    "feature_cols = [c for c in ['lap_sec', 'tyre_age', 'lap_delta', 'rolling_mean_3', 'Compound_enc'] if c in df.columns or c.endswith('_enc')]\n",
    "feature_cols = [c for c in df.columns if c.endswith('_enc')] + [c for c in ['lap_sec', 'tyre_age', 'lap_delta', 'rolling_mean_3'] if c in df.columns]\n",
    "feature_cols = [c for c in feature_cols if c in df.columns]\n",
    "print('Using features:', feature_cols)\n",
    "\n",
    "# -------------------------\n",
    "# Pit-next classifier\n",
    "# -------------------------\n",
    "df_clf = df.dropna(subset=['pit_next'])\n",
    "X = df_clf[feature_cols].fillna(0)\n",
    "y = df_clf['pit_next'].astype(int)\n",
    "\n",
    "# Handle class imbalance using SMOTE or scale_pos_weight\n",
    "print('Class distribution:', y.value_counts())\n",
    "\n",
    "# Simple train-test split for final eval\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Option: SMOTE (careful with time-series leakage; ensure you only apply to training set)\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_train, y_train)\n",
    "print('After resampling, class distribution:', np.bincount(y_res))\n",
    "\n",
    "# Optuna objective for XGBoost binary\n",
    "def objective_clf(trial):\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'booster': 'gbtree',\n",
    "        'tree_method': 'hist',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-3, 10.0),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-3, 10.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "    }\n",
    "    cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    aucs = []\n",
    "    for train_idx, val_idx in cv.split(X_res, y_res):\n",
    "        dtrain = xgb.DMatrix(X_res.iloc[train_idx], label=y_res[train_idx])\n",
    "        dval = xgb.DMatrix(X_res.iloc[val_idx], label=y_res[val_idx])\n",
    "        bst = xgb.train(param, dtrain, num_boost_round=1000, evals=[(dval,'val')], early_stopping_rounds=25, verbose_eval=False)\n",
    "        pred = bst.predict(dval)\n",
    "        aucs.append(roc_auc_score(y_res[val_idx], pred))\n",
    "    return np.mean(aucs)\n",
    "\n",
    "import optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective_clf, n_trials=40)\n",
    "print('Best params:', study.best_params)\n",
    "\n",
    "# Train final model on full resampled train set with best params\n",
    "best_params = study.best_params\n",
    "best_params.update({'verbosity':0, 'objective':'binary:logistic', 'eval_metric':'auc', 'booster':'gbtree','tree_method':'hist'})\n",
    "dtrain_full = xgb.DMatrix(X_res, label=y_res)\n",
    "dtest_full = xgb.DMatrix(X_test, label=y_test)\n",
    "bst_clf = xgb.train(best_params, dtrain_full, num_boost_round=1000, evals=[(dtest_full,'test')], early_stopping_rounds=25)\n",
    "\n",
    "# Evaluate\n",
    "probs = bst_clf.predict(dtest_full)\n",
    "auc = roc_auc_score(y_test, probs)\n",
    "pred = (probs > 0.5).astype(int)\n",
    "print('Test AUC:', auc)\n",
    "print('Precision/Recall/F1:', precision_recall_fscore_support(y_test, pred, average='binary'))\n",
    "\n",
    "# -------------------------\n",
    "# Tyre degradation regressor\n",
    "# -------------------------\n",
    "df_reg = df.dropna(subset=['next_lap_delta'])\n",
    "Xr = df_reg[feature_cols].fillna(0)\n",
    "yr = df_reg['next_lap_delta'].astype(float)\n",
    "\n",
    "# Train-test split\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr, test_size=0.2, random_state=42)\n",
    "\n",
    "def objective_reg(trial):\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'booster': 'gbtree',\n",
    "        'tree_method': 'hist',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-3, 10.0),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-3, 10.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "    }\n",
    "    cv = 4\n",
    "    cv_scores = []\n",
    "    kf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    # use a simple grouping for CV: stratify by sign of yr to create folds (not perfect)\n",
    "    y_sign = (yr_train > 0).astype(int)\n",
    "    for train_idx, val_idx in StratifiedKFold(n_splits=4, shuffle=True, random_state=42).split(Xr_train, y_sign):\n",
    "        dtr = xgb.DMatrix(Xr_train.iloc[train_idx], label=yr_train.iloc[train_idx])\n",
    "        dval = xgb.DMatrix(Xr_train.iloc[val_idx], label=yr_train.iloc[val_idx])\n",
    "        bst = xgb.train(param, dtr, num_boost_round=1000, evals=[(dval,'val')], early_stopping_rounds=25, verbose_eval=False)\n",
    "        pred = bst.predict(dval)\n",
    "        cv_scores.append(mean_squared_error(yr_train.iloc[val_idx], pred, squared=False))\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "study_r = optuna.create_study(direction='minimize')\n",
    "study_r.optimize(objective_reg, n_trials=40)\n",
    "print('Best reg params:', study_r.best_params)\n",
    "\n",
    "# Train final regressor\n",
    "best_r = study_r.best_params\n",
    "best_r.update({'verbosity':0, 'objective':'reg:squarederror', 'eval_metric':'rmse', 'booster':'gbtree','tree_method':'hist'})\n",
    "dtrain_r = xgb.DMatrix(Xr_train, label=yr_train)\n",
    "dtest_r = xgb.DMatrix(Xr_test, label=yr_test)\n",
    "bst_reg = xgb.train(best_r, dtrain_r, num_boost_round=1000, evals=[(dtest_r,'test')], early_stopping_rounds=25)\n",
    "yr_pred = bst_reg.predict(dtest_r)\n",
    "print('Reg RMSE:', mean_squared_error(yr_test, yr_pred, squared=False))\n",
    "print('Reg MAE:', mean_absolute_error(yr_test, yr_pred))\n",
    "\n",
    "# Save models\n",
    "import joblib\n",
    "MODEL_DIR = DATA_DIR / 'models'\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "bst_clf.save_model(str(MODEL_DIR / 'xgb_pit_clf.json'))\n",
    "bst_reg.save_model(str(MODEL_DIR / 'xgb_degradation.json'))\n",
    "print('Models saved to', MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb830ec8",
   "metadata": {},
   "source": [
    "\n",
    "## Part 4 — Sequence models (LSTM/GRU) and Simulator Integration\n",
    "This section trains an LSTM to forecast next-lap times using sequences of past lap times and other features.\n",
    "Then we use the learned predictor inside a Monte-Carlo simulator to evaluate candidate pit strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8cfd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Masking, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Build sequences per stint\n",
    "FEATURES_PATH = DATA_DIR / 'features_parquet.parquet'\n",
    "df = pd.read_parquet(FEATURES_PATH)\n",
    "\n",
    "# We'll create sequences of lap_sec per stint, with optional compound & tyre age features\n",
    "SEQ_LEN = 20\n",
    "seqs = []\n",
    "targets = []\n",
    "meta = []\n",
    "for (driver, stint), g in df.groupby(['Driver','Stint']):\n",
    "    g = g.sort_values('LapNumber')\n",
    "    arr = g['lap_sec'].values\n",
    "    if len(arr) < 3:\n",
    "        continue\n",
    "    for i in range(1, len(arr)):\n",
    "        start = max(0, i-SEQ_LEN)\n",
    "        seq = arr[start:i]\n",
    "        if len(seq) < SEQ_LEN:\n",
    "            seq = np.pad(seq, (SEQ_LEN-len(seq), 0), 'constant', constant_values=0)\n",
    "        seqs.append(seq)\n",
    "        targets.append(arr[i])  # predict absolute next lap time\n",
    "        meta.append({'driver': driver, 'stint': stint})\n",
    "X = np.array(seqs).reshape(-1, SEQ_LEN, 1)\n",
    "y = np.array(targets)\n",
    "\n",
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# Scale data per feature\n",
    "sc = StandardScaler()\n",
    "# fit on flattened train\n",
    "X_train_flat = X_train.reshape(-1, X_train.shape[-1])\n",
    "sc.fit(X_train_flat)\n",
    "def scale_X(x):\n",
    "    flat = x.reshape(-1, x.shape[-1])\n",
    "    flat_s = sc.transform(flat)\n",
    "    return flat_s.reshape(x.shape)\n",
    "\n",
    "X_train_s = scale_X(X_train)\n",
    "X_val_s = scale_X(X_val)\n",
    "\n",
    "# LSTM model\n",
    "model = Sequential([\n",
    "    Masking(mask_value=0., input_shape=(SEQ_LEN,1)),\n",
    "    LSTM(128, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "es = EarlyStopping(patience=6, restore_best_weights=True)\n",
    "mc = ModelCheckpoint(str(DATA_DIR / 'models' / 'lstm_best.h5'), save_best_only=True, monitor='val_loss')\n",
    "model.fit(X_train_s, y_train, validation_data=(X_val_s, y_val), epochs=50, batch_size=128, callbacks=[es, mc])\n",
    "\n",
    "# Example: integrate into simple simulator\n",
    "def predict_next_lap_time_from_sequence(last_seq):\n",
    "    # last_seq should be array of length SEQ_LEN\n",
    "    arr = np.array(last_seq).reshape(1, SEQ_LEN, 1)\n",
    "    arr_s = scale_X(arr)\n",
    "    return float(model.predict(arr_s)[0,0])\n",
    "\n",
    "# Simple Monte Carlo simulator for candidate pit laps\n",
    "def monte_carlo_simulate(current_seq, current_lap, remaining_laps, pit_candidates, pit_loss=22.0, trials=50):\n",
    "    results = {p: [] for p in pit_candidates}\n",
    "    for t in range(trials):\n",
    "        for p in pit_candidates:\n",
    "            seq = list(current_seq)\n",
    "            total = 0.0\n",
    "            lap = current_lap\n",
    "            # simulate until pit\n",
    "            while lap < p:\n",
    "                pred = predict_next_lap_time_from_sequence(seq[-SEQ_LEN:])\n",
    "                total += pred\n",
    "                seq.append(pred)\n",
    "                lap += 1\n",
    "            # pit loss\n",
    "            total += pit_loss\n",
    "            # after pit, assume fresh tyre with faster times (simplified)\n",
    "            for lap_after in range(lap, current_lap + remaining_laps + 1):\n",
    "                # use model with reset sequence (e.g., duplicated last few fresh laps)\n",
    "                pred = predict_next_lap_time_from_sequence(seq[-SEQ_LEN:])\n",
    "                total += pred\n",
    "                seq.append(pred)\n",
    "            results[p].append(total)\n",
    "    # return expected total times per candidate\n",
    "    return {p: np.mean(results[p]) for p in pit_candidates}\n",
    "\n",
    "# Example usage (requires a `current_seq` of recent lap times)\n",
    "print('LSTM training complete; use monte_carlo_simulate to compare candidate pit laps.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ea13d1",
   "metadata": {},
   "source": [
    "\n",
    "## Part 5 — Explainability (SHAP) & Calibration\n",
    "Use SHAP to explain tree-based models (XGBoost). For classifier probability calibration, use isotonic or Platt scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb03fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import shap\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Load model (example path)\n",
    "MODEL_DIR = DATA_DIR / 'models'\n",
    "clf_path = MODEL_DIR / 'xgb_pit_clf.json'\n",
    "if clf_path.exists():\n",
    "    bst = xgb.Booster()\n",
    "    bst.load_model(str(clf_path))\n",
    "    # Wrap as sklearn estimator for SHAP TreeExplainer convenience\n",
    "    # Use shap.TreeExplainer on xgboost booster directly\n",
    "    # To compute SHAP values:\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    explainer = shap.TreeExplainer(bst)\n",
    "    shap_values = explainer.shap_values(dtest)\n",
    "    # summary\n",
    "    try:\n",
    "        shap.summary_plot(shap_values, X_test)\n",
    "    except Exception as e:\n",
    "        print('SHAP plotting may require graphical backend (use Colab/Local). Error:', e)\n",
    "else:\n",
    "    print('Classifier model not found; run Part 3 to train and save models.')\n",
    "\n",
    "# Probability calibration (Platt scaling example)\n",
    "# If you trained an XGBClassifier via sklearn API, use CalibratedClassifierCV\n",
    "# clf_sklearn = xgb.XGBClassifier(**best_params)\n",
    "# clf_cal = CalibratedClassifierCV(clf_sklearn, cv=3, method='isotonic')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbccae08",
   "metadata": {},
   "source": [
    "\n",
    "## Part 6 — Deployment, CI, and Next Steps\n",
    "This section outlines how to:\n",
    "- Serve model via FastAPI + Docker\n",
    "- Create a real-time ingestion pipeline (OpenF1 WebSocket / FastF1 caching)\n",
    "- Continuous evaluation and backtesting: rolling windows, leaderboard, and benchmark scripts.\n",
    "\n",
    "Also includes a checklist for improving performance to approach target accuracy:\n",
    "- Use cross-season training and feature-store with more tracks/seasons\n",
    "- Augment features: telemetry-derived corner/straight speeds, DRS usage, fuel proxies\n",
    "- Ensembling (stacking multiple models) and careful leakage-free CV\n",
    "- Hyperparameter budget increase (Optuna 200+ trials)\n",
    "- Domain adaptation / transfer learning for new circuits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cac37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Minimal FastAPI app scaffold (save as app.py)\n",
    "fastapi_app = r\"\"\"\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class State(BaseModel):\n",
    "    driver: str\n",
    "    lap_number: int\n",
    "    tyre_age: int\n",
    "    lap_sec: float\n",
    "    compound: str\n",
    "\n",
    "# load model artifacts\n",
    "# clf = joblib.load('models/pit_clf.joblib')\n",
    "\n",
    "@app.post('/pit_reco')\n",
    "def pit_reco(state: State):\n",
    "    # implement featurization and model prediction here\n",
    "    return {'prob_pit_next': 0.12, 'explanation': {'tyre_age': 0.4, 'lap_delta': -0.1}}\n",
    "\"\"\"\n",
    "\n",
    "with open('/content/fastapi_example.py', 'w') as f:\n",
    "    f.write(fastapi_app)\n",
    "print('FastAPI scaffold written to /content/fastapi_example.py (adjust paths and load models).')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
